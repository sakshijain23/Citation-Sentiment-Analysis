Learning Global Features for Coreference Resolution

arXiv:1604.03035v1 [cs.CL] 11 Apr 2016

Sam Wiseman and Alexander M. Rush and Stuart M. Shieber
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA, USA
{swiseman,srush,shieber}@seas.harvard.edu

Abstract
There is compelling evidence that coreference prediction would benefit from modeling
global information about entity-clusters. Yet,
state-of-the-art performance can be achieved
with systems treating each mention prediction
independently, which we attribute to the inherent difficulty of crafting informative clusterlevel features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters
directly from their mentions. We show that
such representations are especially useful for
the prediction of pronominal mentions, and
can be incorporated into an end-to-end coreference system that outperforms the state of the
art without requiring any additional search.

1

Introduction

While structured, non-local coreference models
would seem to hold promise for avoiding many common coreference errors (as discussed further in Section 3), the results of employing such models in
practice are decidedly mixed, and state-of-the-art
results can be obtained using a completely local,
mention-ranking system.
In this work, we posit that global context is indeed
necessary for further improvements in coreference
resolution, but argue that informative cluster, rather
than mention, level features are very difficult to devise, limiting their effectiveness. Accordingly, we
instead propose to learn representations of mention
clusters by embedding them sequentially using a recurrent neural network (shown in Section 4). Our
model has no manually defined cluster features, but

instead learns a global representation from the individual mentions present in each cluster. We incorporate these representations into a mention-ranking
style coreference system.
The entire model, including the recurrent neural network and the mention-ranking sub-system, is
trained end-to-end on the coreference task. We train
the model as a local classifier with fixed context (that
is, as a history-based model). As such, unlike several
recent approaches, which may require complicated
inference during training, we are able to train our
model in much the same way as a vanilla mentionranking model.
Experiments compare the use of learned global
features to several strong baseline systems for coreference resolution. We demonstrate that the learned
global representations capture important underlying
information that can help resolve difficult pronominal mentions, which remain a persistent source of
errors for modern coreference systems (Durrett and
Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our
final system improves over 0.8 points in CoNLL
score over the current state of the art, and the improvement is statistically significant on all three
CoNLL metrics.

2

Background and Notation

Coreference resolution is fundamentally a clustering
task. Given a sequence (xn )N
n=1 of (intra-document)
mentions ‚Äì that is, syntactic units that can refer or be
referred to ‚Äì coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M
m=1
such that all the mentions in any particular cluster

X (m) refer to the same underlying entity. Since the
mentions within a particular cluster may be ordered
linearly by their appearance in the document,1 we
(m)
will use the notation Xj to refer to the j‚Äôth mention in the m‚Äôth cluster.
A valid clustering places each mention in exactly
one cluster, and so we may represent a clustering
with a vector z ‚àà {1, . . . , M }N , where zn = m iff
xn is a member of X (m) . Coreference systems attempt to find the best clustering z ‚àó ‚àà Z under some
scoring function, with Z the set of valid clusterings.
One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for
each mention xn ; because xn may not be anaphoric
(and therefore have no antecedents), a ‚Äúdummy‚Äù antecedent  may also be predicted. The aforementioned strategy is adopted by ‚Äúmention-ranking‚Äù systems (Denis and Baldridge, 2008; Rahman and Ng,
2009; Durrett and Klein, 2013), which, formally,
predict an antecedent yÃÇ ‚àà Y(xn ) for each mention
xn , where Y(xn ) = {1, . . . , n ‚àí 1, }. Through transitivity, these decisions induce a clustering over the
document.
Mention-ranking systems make their antecedent
predictions with a local scoring function f (xn , y)
defined for any mention xn and any antecedent
y ‚àà Y(xn ). While such a scoring function clearly
ignores much structural information, the mentionranking approach has been attractive for at least two
reasons. First, inference is relatively simple and efficient, requiring only a left-to-right pass through a
document‚Äôs mentions during which a mention‚Äôs antecedents (as well as ) are scored and the highest
scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention
xn to be compatible with only one of its coreferent
antecedents. This contrasts with mention-pair models (e.g., Bengtson and Roth (2008)), which score
all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta
et al. (2007)). Modeling each mention as having
a single antecedent is particularly advantageous for
pronominal mentions, which we might like to model
1

We assume nested mentions are ordered by their syntactic
heads.

as linking to a single nominal or proper antecedent,
for example, but not necessarily to all other coreferent mentions.
Accordingly, in this paper we attempt to maintain
the inferential simplicity and modeling benefits of
mention ranking, while allowing the model to utilize
global, structural information relating to z in making its predictions. We therefore investigate objective functions of the form
arg max
y1 ,...,yN

N
X

f (xn , yn ) + g(xn , yn , z 1:n‚àí1 )

,

n=1

where g is a global function that, in making predictions for xn , may examine (features of) the clustering z 1:n‚àí1 induced by the antecedent predictions
made through yn‚àí1 .

3

The Role of Global Features

Here we motivate the use of global features for
coreference resolution by focusing on the issues that
may arise when resolving pronominal mentions in
a purely local way. See Clark and Manning (2015)
and Stoyanov and Eisner (2012) for more general
motivation for using global models.
3.1

Pronoun Problems

Recent empirical work has shown that the resolution of pronominal mentions accounts for a substantial percentage of the total errors made by modern
mention-ranking systems. Wiseman et al. (2015)
show that on the CoNLL 2012 English development
set, almost 59% of mention-ranking precision errors
and almost 24% of recall errors involve pronominal
mentions. Martschat and Strube (2015) found a similar pattern in their comparison of mention-ranking,
mention-pair, and latent-tree models.
To see why pronouns can be so problematic, consider the following passage from the ‚ÄúBroadcast
Conversation‚Äù portion of the CoNLL development
set (bc/msnbc/0000/018); below, we enclose mentions in brackets and give the same subscript to coclustered mentions. (This example is also shown in
Figure 2.)
DA: um and [I]1 think that is what‚Äôs - Go
ahead [Linda]2 .
LW: Well and uh thanks goes to [you]1 and to
[the media]3 to help [us]4 ...So [our]4 hat is off
to all of [you]5 as well.

This example is typical of Broadcast Conversation,
and it is difficult because local systems learn to myopically link pronouns such as [you]5 to other instances of the same pronoun that are close by, such
as [you]1 . While this is often a reasonable strategy,
in this case predicting [you]1 to be an antecedent of
[you]5 would result in the prediction of an incoherent cluster, since [you]1 is coreferent with the singular [I]1 , and [you]5 , as part of the phrase ‚Äúall of you,‚Äù
is evidently plural. Thus, while there is enough information in the text to correctly predict [you]5 , doing so crucially depends on having access to the history of predictions made so far, and it is precisely
this access to history that local models lack.
More empirically, there are non-local statistical
regularities involving pronouns we might hope models could exploit. For instance, in the CoNLL training data over 70% of pleonastic ‚Äúit‚Äù instances and
over 74% of pleonastic ‚Äúyou‚Äù instances follow (respectively) previous pleonastic ‚Äúit‚Äù and ‚Äúyou‚Äù instances. Similarly, over 78% of referential ‚ÄúI‚Äù instances and over 68% of referential ‚Äúhe‚Äù instances
corefer with previous ‚ÄúI‚Äù and ‚Äúhe‚Äù instances, respectively.
Accordingly, we might expect non-local models
with access to global features to perform significantly better. However, models incorporating nonlocal features have a rather mixed track record. For
instance, BjoÃàrkelund and Kuhn (2014) found that
cluster-level features improved their results, whereas
Martschat and Strube (2015) found that they did not.
Clark and Manning (2015) found that incorporating
cluster-level features beyond those involving the precomputed mention-pair and mention-ranking probabilities that form the basis of their agglomerative
clustering coreference system did not improve performance. Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are
completely local) perform at least as well as their
more structured counterparts (Durrett and Klein,
2014; Clark and Manning, 2015; Wiseman et al.,
2015; Peng et al., 2015).
3.2

Issues with Global Features

We believe a major reason for the relative ineffectiveness of global features in coreference problems is that, as noted by Clark and Manning (2015),
cluster-level features can be hard to define. Specif-

ically, it is difficult to define discrete, fixed-length
features on clusters, which can be of variable size
(or shape). As a result, global coreference features
tend to be either too coarse or too sparse. Thus, early
attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most
to the mention-level features defined on the mentions (or pairs of mentions) in a cluster (Culotta et
al., 2007; Rahman and Ng, 2011). For example, a
cluster would have the feature ‚Äòmost-female=true‚Äô if
more than half the mentions (or pairs of mentions)
in the cluster have a ‚Äòfemale=true‚Äô feature.
On the other extreme, BjoÃàrkelund and Kuhn
(2014) define certain cluster-level features by concatenating the mention-level features of a cluster‚Äôs
constituent mentions in order of the mentions‚Äô appearance in the document. For example, if a cluster consists, in order, of the mentions (the president,
he, he), they would define a cluster-level ‚Äútype‚Äù feature ‚ÄòC-P-P=true‚Äô, which indicates that the cluster is
composed, in order, of a common noun, a pronoun,
and a pronoun. While very expressive, these concatenated features are often quite sparse, since clusters encountered during training can be of any size.

4

Learning Global Features

To circumvent the aforementioned issues with defining global features, we propose to learn cluster-level
feature representations implicitly, by identifying the
state of a (partial) cluster with the hidden state of
an RNN that has consumed the sequence of mentions composing the (partial) cluster. Before providing technical details, we provide some preliminary evidence that such learned representations capture important contextual information by displaying in Figure 1 the learned final states of all clusters in the CoNLL development set, projected using
T-SNE (van der Maaten and Hinton, 2012). Each
point in the visualization represents the learned features for an entity cluster and the head words of
mentions are shown for representative points. Note
that the model learns to roughly separate clusters by
simple distinctions such as predominant type (nominal, proper, pronominal) and number (it, they, etc),
but also captures more subtle relationships such as
grouping geographic terms and long strings of pronouns.

der). We therefore propose to embed the state(s) of
X (m) by running an RNN over the cluster in order.
In order to run an RNN over the mentions we need
an embedding function hc to map a mention to a real
vector. First, following Wiseman et al. (2015) define
œÜa (xn ) : X ‚Üí {0, 1}F as a standard set of local indicator features on a mention, such as its head word,
its gender, and so on. (We elaborate on features below.) We then use a non-linear feature embedding
hc to map a mention xn to a vector-space representation. In particular, we define
hc (xn ) , tanh(W c œÜa (xn ) + bc )

Figure 1: T-SNE visualization of learned entity representations on the CoNLL development set. Each point
shows a gold cluster of size > 1. Yellow, red, and purple points represent predominantly common noun, proper
noun, and pronoun clusters, respectively. Captions show
head words of representative clusters‚Äô mentions.

4.1

Recurrent Neural Networks

A recurrent neural network is a parameterized nonlinear function RNN that recursively maps an input sequence of vectors to a sequence of hidden
states. Let (mj )Jj=1 be a sequence of J input vectors mj ‚àà RD , and let h0 = 0. Applying an RNN to
any such sequence yields
hj ‚Üê RNN(mj , hj‚àí1 ; Œ∏)

where W c and bc are parameters of the embedding.
We will refer to the j‚Äôth hidden state of the RNN
(m)
corresponding to X (m) as hj , and we obtain it according to the following formula
(m)

hj

(m)

‚Üê RNN(hc (Xj

(m)

), hj‚àí1 ; Œ∏)

,

(m)

again assuming that h0 = 0. Thus, we will effectively run an RNN over each (sequence of mentions corresponding to a) cluster X (m) in the docu(m)
ment, and thereby generate a hidden state hj corresponding to each step of each cluster in the document. Concretely, this can be implemented by maintaining M RNNs ‚Äì one for each cluster ‚Äì that all
share the parameters Œ∏. The process is illustrated in
the top portion of Figure 2.

5

Coreference with Global Features

,

where Œ∏ is the set of parameters for the model, which
are shared over time.
There are several varieties of RNN, but by far
the most commonly used in natural-language processing is the Long Short-Term Memory network
(LSTM) (Hochreiter and Schmidhuber, 1997), particularly for language modeling (e.g., Zaremba et al.
(2014)) and machine translation (e.g., Sutskever et
al. (2014)), and we use LSTMs in all experiments.
4.2

,

RNNs for Cluster Features

Our main contribution will be to utilize RNNs to
produce feature representations of entity clusters
which will provide the basis of the global term g.
Recall that we view a cluster X (m) as a sequence of
(m)
mentions (Xj )Jj=1 (ordered in linear document or-

We now describe how the RNN defined above is
used within an end-to-end coreference system.
5.1 Full Model and Training
Recall that our inference objective is to maximize
the score of both a local mention ranking term as
well as a global term based on the current clusters:
arg max
y1 ,...,yN

N
X

f (xn , yn ) + g(xn , yn , z 1:n‚àí1 )

n=1

We begin by defining the local model f (xn , y)
with the two layer neural network of Wiseman et
al. (2015), which has a specialization for the nonanaphoric case, as follows:
(
f (xn , y) ,

uT

h

ha (xn )
hp (xn ,y)

i

+ u0

v T ha (xn ) + v0

if y 6= 
if y = 

.

DA: um and [I]1 think that is what‚Äôs - Go ahead [Linda]2 .
LW: Well and thanks goes to [you]1 and to [the media]3 to help [us]4 ...So [our]4 hat is off to all of [you]5 ...
X (2)

X (3)

(1)
h1

(1)
h2

(2)
h1

(3)
h1

(4)
h1

h2

[I]

[you]

[Linda]

[the media]

[us]

[our]

X (1)

(1)

[I], h2

(2)

[Linda], h1

(1)

[you], h2

(3)

[the media], h1

X (4)

(4)

[us], h2

(4)

[our], h2

(4)

xn = [you] , NA(xn )

Figure 2: Full RNN example for handling the mention xn = [you]. There are currently four entity clusters in scope
X (1) , X (2) , X (3) , X (4) based on unseen previous decisions (y). Each cluster has a corresponding RNN state, two
of which (h(1) and h(4) ) have processed multiple mentions (with X (1) notably including a singular mention [I]). At
the bottom, we show the complete mention-ranking process. Each previous mention is considered as an antecedent,
and the global term considers the antecedent clusters‚Äô current hidden state. Selecting  is treated with a special case
NA(xn ).

Above, u and v are the parameters of the model,
and ha and hp are learned feature embeddings of
the local mention context and the pairwise affinity
between a mention and an antecedent, respectively.
These feature embeddings are defined similarly to
hc , as
ha (xn ) , tanh(W a œÜa (xn ) + ba )
hp (xn , y) , tanh(W p œÜp (xn , y) + bp )

,

where œÜa (mentioned above) and œÜp are ‚Äúraw‚Äù (that
is, unconjoined) features on the context of xn and
on the pairwise affinity between mentions xn and
antecedent y, respectively (Wiseman et al., 2015).
Note that ha and hc use the same raw features; only
their weights differ.
We now specify our global scoring function g
based on the history of previous decisions. Define
(m)
h<n as the hidden state of cluster m before a de(m)
cision is made for xn ‚Äì that is, h<n is the state of
cluster m‚Äôs RNN after it has consumed all mentions
in the cluster preceding xn . We define g as
(
(z )
hc (xn )T h<ny
g(xn , y,z 1:n‚àí1 ) ,
NA(xn )

if y =
6 
if y = 

,

where NA gives a score for assigning  based on
a non-linear function of all of the current hidden
states:

h œÜ (x ) i

n
NA(xn ) = q T tanh W s PMa h(m) + bs .
m=1

<n

See Figure 2 for a diagram. The intuition behind
the first case in g is that in considering whether y
is a good antecedent for xn , we add a term to the
score that examines how well xn matches with the
mentions already in X (zy ) ; this matching score is expressed via a dot-product.2 In the second case, when
predicting that xn is non-anaphoric, we add the NA
term to the score, which examines the (sum of) the
(m)
current states h<n of all clusters. This information
is useful both because it allows the non-anaphoric
score to incorporate information about potential antecedents, and because the occurrence of certain
singleton-clusters often predicts the occurrence of
future singleton-clusters, as noted in Section 3.
The whole system is trained end-to-end on coreference using backpropagation. For a given training
document, let z (o) be the oracle mapping from mention to cluster, which induces an oracle clustering.
While at training time we do have oracle clusters, we
do not have oracle antecedents (y)N
n=1 , so following
past work we treat the oracle antecedent as latent (Yu
and Joachims, 2009; Fernandes et al., 2012; Chang
et al., 2013; Durrett and Klein, 2013). We train with
the following slack-rescaled, margin objective:

2

We also experimented with other non-linear functions, but
dot-products performed best.

Algorithm 1 Greedy search with global RNNs
N
X
n=1

max ‚àÜ(xn , yÃÇ)(1 + f (xn , yÃÇ) + g(xn , yÃÇ, z (o) )

yÃÇ‚ààY(xn )

‚àí f (xn , yn` ) ‚àí g(xn , yn` , z (o) )),

where the latent antecedent yn` is defined as
yn` ,

arg max

f (xn , y) + g(xn , y, z (o) )

(o)
(o)
y‚ààY(xn ):zy =zn

if xn is anaphoric, and is  otherwise. The term
‚àÜ(xn , yÃÇ) gives different weight to different error types. We use a ‚àÜ with 3 different weights
(Œ±1 , Œ±2 , Œ±3 ) for ‚Äúfalse link‚Äù (FL), ‚Äúfalse new‚Äù (FN),
and ‚Äúwrong link‚Äù (WL) mistakes (Durrett and Klein,
2013), which correspond to predicting an antecedent
when non-anaphoric,  when anaphoric, and the
wrong antecedent, respectively.
Note that in training we use the oracle clusters
z (o) . Since these are known a priori, we can pre(m)
compute all the hidden states hj in a document,
which makes training quite simple and efficient.
This approach contrasts in particular with the work
of BjoÃàrkelund and Kuhn (2014) ‚Äî who also incorporate global information in mention-ranking ‚Äî in
that they train against latent trees, which are not annotated and must be searched for during training. On
the other hand, training on oracle clusters leads to a
mismatch between training and test, which can hurt
performance.
5.2

Search

When moving from a strictly local objective to one
with global features, the test-time search problem
becomes intractable. The local objective requires
O(n2 ) time, whereas the full clustering problem is
NP-Hard. Past work with global features has used
integer linear programming solvers for exact search
(Chang et al., 2013; Peng et al., 2015), or beam
search with (delayed) early update training for an
approximate solution (BjoÃàrkelund and Kuhn, 2014).
In contrast, we simply use greedy search at test time,
which also requires O(n2 ) time.3 The full algorithm
3
While beam search is a natural way to decrease search error at test time, it may fail to help if training involves a local
margin objective (as in our case), since scores need not be calibrated across local decisions. We accordingly attempted to train
various locally normalized versions of our model, but found that

1: procedure G REEDY C LUSTER(x1 , . . . , xN )
2:
Initialize clusters X (1) . . . as empty lists, hidden states
(0)
h , . . . as 0 vectors in RD , z as map from mention to
cluster, and cluster counter M ‚Üê 0
3:
for n = 2 . . . N do
4:
y ‚àó ‚Üê arg max f (xn , y) + g(xn , y, z 1:n‚àí1 )
y‚ààY(xn )

5:
6:
7:
8:
9:
10:
11:
12:

m ‚Üê zy ‚àó
if y ‚àó =  then
M ‚ÜêM +1
m‚ÜêM
append xn to X (m)
zn ‚Üê m
h(m) ‚Üê RNN(hc (xn ), h(m) )
return X (1) , . . . , X (M )

is shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates
the current RNN representation based on the previous decision that was made, and line 4, which then
uses this cluster representation as part of scoring.

6

Experiments

6.1

Methods

We run experiments on the CoNLL 2012 English
shared task (Pradhan et al., 2012). The task uses
the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the
shared task. For all experiments, we use the Berkeley Coreference System (Durrett and Klein, 2013)
for mention extraction and to compute features œÜa
and œÜp .
Features We use the raw BASIC + feature sets described by Wiseman et al. (2015), with the following
modifications:
‚Ä¢ We remove all features from œÜp that concatenate a feature of the antecedent with a feature of
the current mention, such as bi-head features.
‚Ä¢ We add true-cased head features, a current
speaker indicator feature, and a 2-character
they underperformed. We also experimented with training approaches and model variants that expose the model to its own
predictions (DaumeÃÅ III et al., 2009; Ross et al., 2011; Bengio et
al., 2015), but found that these yielded a negligible performance
improvement.

System
B&K (2014)
M&S (2015)
C&M (2015)
Peng et al. (2015)
Wiseman et al. (2015)
This work

P

MUC
R

F1

74.30
76.72
76.12
76.23
77.49

67.46
68.13
69.38
69.31
69.75

70.72
72.17
72.59
72.22
72.60
73.42

P

B3
R

F1

P

CEAFe
R

F1

CoNLL

62.71
66.12
65.64
66.07
66.83

54.96
54.22
56.01
55.83
56.95

58.58
59.58
60.44
60.50
60.52
61.50

59.40
59.47
59.44
59.41
62.14

52.27
52.33
52.98
54.88
53.85

55.61
55.67
56.02
56.37
57.05
57.70

61.63
62.47
63.02
63.03
63.39
64.21

Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in
order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and
Wiseman et al. (2015). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared
with Wiseman et al. (2015) for all metrics.

genre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to œÜp and œÜa .
‚Ä¢ We add features indicating if a mention has a
substring overlap with the current speaker (œÜp
and œÜa ), and if an antecedent has a substring
overlap with a speaker distinct from the current
mention‚Äôs speaker (œÜp ).
‚Ä¢ We add a single centered, rescaled document
position feature to each mention when learning
hc . We calculate a mention xn ‚Äôs rescaled doc‚àí1
ument position as 2n‚àíN
N ‚àí1 .
These modifications result in there being approximately 14K distinct features in œÜa and approximately 28K distinct features in œÜp , which is far
fewer features than has been typical in past work.
For training, we use document-size minibatches,
which allows for efficient pre-computation of RNN
states, and we minimize the loss described in Section 5 with AdaGrad (Duchi et al., 2011) (after
clipping LSTM gradients to lie (elementwise) in
(‚àí10, 10)). We find that the initial learning rate chosen for AdaGrad has a significant impact on results,
and we choose learning rates for each layer out of
{0.1, 0.02, 0.01, 0.002, 0.001}.
In experiments, we set ha (xn ), hc (xn ), and h(m)
to be ‚àà R200 , and hp (xn , y) ‚àà R700 . We use a
single-layer LSTM (without ‚Äúpeep-hole‚Äù connections), as implemented in the element-rnn library (LeÃÅonard et al., 2015). For regularization,
we apply Dropout (Srivastava et al., 2014) with a
rate of 0.4 before applying the linear weights u,
and we also apply Dropout with a rate of 0.3 to the
LSTM states before forming the dot-product scores.

MR
Avg, OH
RNN, GH
RNN, OH

MUC

B3

CEAFe

CoNLL

73.06
73.30
73.63
74.26

62.66
63.06
63.23
63.89

58.98
58.85
59.56
59.54

64.90
65.07
65.47
65.90

Table 2: F1 scores of models described in text on CoNLL
2012 development set. Rows in grey highlight models
using oracle history.

Following Wiseman et al. (2015) we use the costweights Œ± = h0.5, 1.2, 1i in defining ‚àÜ, and we
use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official
CoNLL 2012 script (Pradhan et al., 2014; Luo et al.,
2014). Code for our system is available at https:
//github.com/swiseman/nn_coref. The
system makes use of a GPU for training, and trains
in about two hours.
6.2

Results

In Table 1 we present our main results on the CoNLL
English test set, and compare with other recent stateof-the-art systems. We see a statistically significant
improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F1 scores to
date on all three CoNLL metrics.
We now consider in more detail the impact of
global features and RNNs on performance. For these
experiments, we report MUC, B3 , and CEAFe F1 scores in Table 2 as well as errors broken down
by mention type and by whether the mention is
anaphoric or not in Table 3. Table 3 further partitions errors into FL, FN, and WL categories, which

Non-Anaphoric (FL)
Nom. HM Nom. No HM
Pron.
MR
Avg, OH
RNN, GH
RNN, OH

1061
1983
1914
1913

1130
1140
1125
1130

1075
1011
1893
1842

# Mentions

9.0K

22.2K

3.1K

Model

Anaphoric (FN + WL)
Nom. HM Nom. No HM
Pron.

MR
Avg, OH
RNN, GH
RNN, OH

665+326
781+300
767+303
750+289

666+56
641+60
648+57
648+52

533+796
578+744
664+727
611+686

# Mentions

4.7K

1.0K

7.3K

Table 3: Number of ‚Äúfalse link‚Äù (FL) errors on nonanaphoric mentions (top) and number of ‚Äúfalse new‚Äù (FN)
and ‚Äúwrong link‚Äù (WL) errors on anaphoric mentions
(bottom) on CoNLL 2012 development set. Mentions
are categorized as nominal or proper with (previous) head
match (Nom. HM), nominal or proper with no head match
(Nom. No HM), and pronominal. Models are described
in the text, and rows in grey highlight models using oracle
history.

are defined in Section 5.1. We typically think of FL
and WL as representing precision errors, and FN as
representing recall errors.
Our experiments consider several different settings.
First, we consider an oracle setting
(‚ÄúRNN, OH‚Äù in tables), in which the model receives
(o)
z 1:n‚àí1 , the oracle partial clustering of all mentions
preceding xn in the document, and is therefore not
forced to rely on its own past predictions when predicting xn . This provides us with an upper bound on
the performance achievable with our model. Next,
we consider the performance of the model under
a greedy inference strategy (RNN, GH), as in Algorithm 1. Finally, for baselines we consider the
mention-ranking system (MR) of Wiseman et al.
(2015) using our updated feature-set, as well as a
non-local baseline with oracle history (Avg, OH),
which averages the representations hc (xj ) for all
xj ‚àà X (m) , rather than feed them through an RNN;
errors are still backpropagated through the hc representations during learning.
In Table 3 we see that the RNN improves performance overall, with the most dramatic improve-

Figure 3: Cluster predictions of greedy RNN model; coclustered mentions are of the same color, and intensity of
(i)
mention xj corresponds to hc (xn )T h<k , where k = j+1,
i ‚àà {1, 2}, and xn = ‚Äúhis.‚Äù See text for full description.

ments on non-anaphoric pronouns, though errors are
also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one
mention with the same head. While WL errors also
decrease for both these mention-categories under the
RNN model, FN errors increase. Importantly, the
RNN performance is significantly better than that
of the Avg baseline, which barely improves over
mention-ranking, even with oracle history. This suggests that modeling the sequence of mentions in a
cluster is advantageous. We also note that while
RNN performance degrades in both precision and
recall when moving from the oracle history upperbound to a greedy setting, we are still able to recover
a significant portion of the possible performance improvement.
6.3

Qualitative Analysis

In this section we consider in detail the impact of the
g term in the RNN scoring function on the two error
categories that improve most under the RNN model
(as shown in Table 3), namely, pronominal WL errors
and pronominal FL errors. We consider an example
from the CoNLL development set in each category
on which the baseline MR model makes an error but
the greedy RNN model does not.
The example in Figure 3 involves the resolution
of the ambiguous pronoun ‚Äúhis,‚Äù which is bracketed and in bold in the figure. Whereas the baseline
MR model incorrectly predicts ‚Äúhis‚Äù to corefer with
the closest gender-consistent antecedent ‚ÄúJustin‚Äù ‚Äî
thus making a WL error ‚Äî the greedy RNN model

which may suggest that earlier, easier predictions of
pleonasm can inform subsequent predictions.

7
Figure 4: Magnitudes of gradients of NA score applied
to bold ‚ÄúIt‚Äôs‚Äù with respect to final mention in three preceding clusters. See text for full description.

correctly predicts ‚Äúhis‚Äù to corefer with ‚ÄúMr. Kaye‚Äù
in the previous sentence. (Note that ‚Äúthe official‚Äù
also refers to Mr. Kaye). To get a sense of the greedy
RNN model‚Äôs decision-making on this example, we
color the mentions the greedy RNN model has predicted to corefer with ‚ÄúMr. Kaye‚Äù in green, and the
mentions it has predicted to corefer with ‚ÄúJustin‚Äù in
blue. (Note that the model incorrectly predicts the
initial ‚ÄúI‚Äù mentions to corefer with ‚ÄúJustin.‚Äù) Letting X (1) refer to the blue cluster, X (2) refer to the
green cluster, and xn refer to the ambiguous mention
‚Äúhis,‚Äù we further shade each mention xj in X (1) so
(1)
that its intensity corresponds to hc (xn )T h<k , where
k = j + 1; mentions in X (2) are shaded analogously.
Thus, the shading shows how highly g scores the
compatibility between ‚Äúhis‚Äù and a cluster X (i) as
each of X (i) ‚Äôs mentions is added. We see that when
the initial ‚ÄúJustin‚Äù mentions are added to X (1) the
g-score is relatively high. However, after ‚ÄúThe company‚Äù is correctly predicted to corefer with ‚ÄúJustin,‚Äù
the score of X (1) drops, since companies are generally not coreferent with pronouns like ‚Äúhis.‚Äù
Figure 4 shows an example (consisting of a telephone conversation between ‚ÄúA‚Äù and ‚ÄúB‚Äù) in which
the bracketed pronoun ‚ÄúIt‚Äôs‚Äù is being used pleonastically. Whereas the baseline MR model predicts
‚ÄúIt‚Äôs‚Äù to corefer with a previous ‚Äúit‚Äù ‚Äî thus making a FL error ‚Äî the greedy RNN model does not. In
Figure 4 the final mention in three preceding clusters
is shaded so its intensity corresponds to the magnitude of the gradient of the NA term in g with respect to that mention. This visualization resembles
the ‚Äúsaliency‚Äù technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score.
We see that the potential antecedent ‚ÄúS-Bahn‚Äù
has a large gradient, but also that the initial, obviously pleonastic use of ‚Äúit‚Äôs‚Äù has a large gradient,

Related Work

In addition to the related work noted throughout,
we add supplementary references here. Unstructured approaches to coreference typically divide into
mention-pair models, which classify (nearly) every
pair of mentions in a document as coreferent or
not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models,
which select a single antecedent for each anaphoric
mention (Denis and Baldridge, 2008; Rahman and
Ng, 2009; Durrett and Klein, 2013; Chang et al.,
2013; Wiseman et al., 2015). Structured approaches
typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003;
Culotta et al., 2007; Poon and Domingos, 2008;
Haghighi and Klein, 2010; Stoyanov and Eisner,
2012; Cai and Strube, 2010), and, more recently,
those that learn a latent tree of mentions (Fernandes
et al., 2012; BjoÃàrkelund and Kuhn, 2014; Martschat
and Strube, 2015).
There have also been structured approaches that
merge the mention-ranking and mention-pair ideas
in some way. For instance, Rahman and Ng (2011)
rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking
and mention pair systems to learn a clustering.
The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel,
though it bears some similarity to the recent work
of Dyer et al. (2015), who use LSTMs to embed the
state of a transition based parser‚Äôs stack.

8

Conclusion

We have presented a simple, state of the art approach
to incorporating global information in an end-to-end
coreference system, which obviates the need to define global features, and moreover allows for simple
(greedy) inference. Future work will examine improving recall, and more sophisticated approaches
to global training.

Acknowledgments
We gratefully acknowledge the support of a Google
Research Award.

References
[Bengio et al.2015] Samy Bengio,
Oriol Vinyals,
Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled
sampling for sequence prediction with recurrent
neural networks. In Advances in Neural Information
Processing Systems, pages 1171‚Äì1179.
[Bengtson and Roth2008] Eric Bengtson and Dan Roth.
2008. Understanding the Value of Features for Coreference Resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 294‚Äì303. Association for Computational Linguistics.
[BjoÃàrkelund and Kuhn2014] Anders BjoÃàrkelund and
Jonas Kuhn. 2014. Learning structured perceptrons
for coreference Resolution with Latent Antecedents
and Non-local Features. ACL, Baltimore, MD, USA,
June.
[Cai and Strube2010] Jie Cai and Michael Strube. 2010.
End-to-end coreference resolution via hypergraph partitioning. In 23rd International Conference on Computational Linguistics (COLING), pages 143‚Äì151.
[Chang et al.2013] Kai-Wei Chang, Rajhans Samdani,
and Dan Roth. 2013. A Constrained Latent Variable
Model for Coreference Resolution. In Proceedings of
the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 601‚Äì612.
[Clark and Manning2015] Kevin Clark and Christopher D. Manning. 2015. Entity-centric coreference
resolution with model stacking.
In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1405‚Äì1415.
[Culotta et al.2007] Aron Culotta, Michael Wick, Robert
Hall, and Andrew McCallum. 2007. First-order Probabilistic Models for Coreference Resolution. In Human Language Technology Conference of the North
American Chapter of the Association of Computational Linguistics (NAACL HLT).
[DaumeÃÅ III et al.2009] Hal DaumeÃÅ III, John Langford,
and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, 75(3):297‚Äì325.
[Denis and Baldridge2008] Pascal Denis and Jason
Baldridge. 2008. Specialized Models and Ranking
for Coreference Resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 660‚Äì669. Association
for Computational Linguistics.
[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Machine Learning Research, 12:2121‚Äì2159.
[Durrett and Klein2013] Greg Durrett and Dan Klein.
2013. Easy Victories and Uphill Battles in Coreference Resolution. In Proceedings of the 2013 Confer-

ence on Empirical Methods in Natural Language Processing, pages 1971‚Äì1982.
[Durrett and Klein2014] Greg Durrett and Dan Klein.
2014. A Joint Model for Entity Analysis: Coreference,
Typing, and Linking. Transactions of the Association
for Computational Linguistics, 2:477‚Äì490.
[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
short-term memory. In Proceedings of the 53rd Annual Meeting of the Association for Computational
Linguistics (ACL), pages 334‚Äì343.
[Fernandes et al.2012] Eraldo
Rezende
Fernandes,
Cƒ±ÃÅcero Nogueira Dos Santos, and Ruy Luiz MilidiuÃÅ.
2012.
Latent Structure Perceptron with Feature
Induction for Unrestricted Coreference Resolution.
In Joint Conference on EMNLP and CoNLL-Shared
Task, pages 41‚Äì48. Association for Computational
Linguistics.
[Haghighi and Klein2010] Aria Haghighi and Dan Klein.
2010. Coreference Resolution in a Modular, Entitycentered Model. In The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 385‚Äì393. Association for Computational Linguistics.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
JuÃàrgen Schmidhuber. 1997. Long short-term memory.
Neural Comput., 9:1735‚Äì1780.
[Hovy et al.2006] Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2006. Ontonotes: the 90% Solution. In Proceedings
of the human language technology conference of the
NAACL, Companion Volume: Short Papers, pages 57‚Äì
60. Association for Computational Linguistics.
[Koehn2004] Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In
Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing, pages 388‚Äì
395. Citeseer.
[Kummerfeld and Klein2013] Jonathan K. Kummerfeld
and Dan Klein. 2013. Error-driven Analysis of Challenges in Coreference Resolution. In Proceedings of
the 2013 Conference on Empirical Methods in Natural
Language Processing, Seattle, WA, USA, October.
[LeÃÅonard et al.2015] Nicholas LeÃÅonard, Yand Waghmare,
Sagar ad Wang, and Jin-Hwa Kim. 2015. rnn:
Recurrent Library for Torch.
arXiv preprint
arXiv:1511.07889.
[Li et al.2016] Jiwei Li, Xinlei Chen, Eduard Hovy, and
Dan Jurafsky. 2016. Visualizing and understanding
neural models in nlp. In NAACL HLT.
[Luo et al.2014] Xiaoqiang Luo, Sameer Pradhan, Marta
Recasens, and Eduard Hovy. 2014. An Extension of

BLANC to System Mentions. Proceedings of ACL,
Baltimore, Maryland, June.
[Martschat and Strube2015] Sebastian Martschat and
Michael Strube. 2015. Latent structures for coreference resolution. TACL, 3:405‚Äì418.
[Martschat et al.2015] Sebastian Martschat,
Thierry
GoÃàckel, and Michael Strube. 2015. Analyzing and
visualizing coreference resolution errors. In NAACL
HLT, pages 6‚Äì10.
[McCallum and Wellner2003] Andrew McCallum and
Ben Wellner. 2003. Toward Conditional Models
of Identity Uncertainty with Application to Proper
Noun Coreference. Advances in Neural Information
Processing Systems 17.
[Ng and Cardie2002] Vincent Ng and Claire Cardie.
2002. Identifying Anaphoric and Non-anaphoric
Noun Phrases to Improve Coreference Resolution. In
Proceedings of the 19th international conference on
Computational linguistics-Volume 1, pages 1‚Äì7. Association for Computational Linguistics.
[Peng et al.2015] Haoruo Peng, Kai-Wei Chang, and Dan
Roth. 2015. A joint framework for coreference resolution and mention head detection. In Proceedings of
the 19th Conference on Computational Natural Language Learning (CoNLL), pages 12‚Äì21.
[Poon and Domingos2008] Hoifung Poon and Pedro M.
Domingos. 2008. Joint unsupervised coreference resolution with markov logic. In 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 650‚Äì659.
[Pradhan et al.2012] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen
Zhang. 2012. Conll-2012 Shared Task: Modeling
Multilingual Unrestricted Coreference in OntoNotes.
In Joint Conference on EMNLP and CoNLL-Shared
Task, pages 1‚Äì40. Association for Computational Linguistics.
[Pradhan et al.2014] Sameer Pradhan, Xiaoqiang Luo,
Marta Recasens, Eduard Hovy, Vincent Ng, and
Michael Strube. 2014. Scoring Coreference Partitions
of Predicted Mentions: A Reference Implementation.
In Proceedings of the Association for Computational
Linguistics.
[Rahman and Ng2009] Altaf Rahman and Vincent Ng.
2009. Supervised Models for Coreference Resolution.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2Volume 2, pages 968‚Äì977. Association for Computational Linguistics.
[Rahman and Ng2011] Altaf Rahman and Vincent Ng.
2011. Narrowing the modeling gap: A cluster-ranking
approach to coreference resolution. J. Artif. Intell. Res.
(JAIR), 40:469‚Äì521.

[Ross et al.2011] SteÃÅphane Ross, Geoffrey J. Gordon, and
Drew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics,
pages 627‚Äì635.
[Soon et al.2001] Wee Meng Soon, Hwee Tou Ng, and
Daniel Chung Yong Lim. 2001. A Machine Learning
Approach to Coreference Resolution of Noun Phrases.
Computational Linguistics, 27(4):521‚Äì544.
[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of
Machine Learning Research, 15(1):1929‚Äì1958.
[Stoyanov and Eisner2012] Veselin Stoyanov and Jason
Eisner. 2012. Easy-first Coreference Resolution. In
COLING, pages 2519‚Äì2534. Citeseer.
[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 3104‚Äì3112.
[van der Maaten and Hinton2012] Laurens
van
der
Maaten and Geoffrey E. Hinton. 2012. Visualizing
non-metric similarities in multiple maps. Machine
Learning, 87(1):33‚Äì55.
[Wiseman et al.2015] Sam Wiseman, Alexander M. Rush,
Stuart M. Shieber, and Jason Weston. 2015. Learning
anaphoricity and antecedent ranking features for coreference resolution. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics (ACL), pages 1416‚Äì1426.
[Yu and Joachims2009] Chun-Nam John Yu and Thorsten
Joachims. 2009. Learning Structural SVMs with Latent Variables. In Proceedings of the 26th Annual International Conference on Machine Learning, pages
1169‚Äì1176. ACM.
[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,
and Oriol Vinyals. 2014. Recurrent neural network
regularization. CoRR, abs/1409.2329.

